{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso & Regularization\n",
    "\n",
    "## Objectives \n",
    "<a name=\"objectives\"></a>\n",
    "\n",
    "- List methods other than r-squared to assess model fit\n",
    "\n",
    "- Describe regularization's role in regression\n",
    "\n",
    "- Summarize the difference between L1 and L2 norms\n",
    "\n",
    "- Understand the effect of hyper-parameter $\\alpha$ in Ridge and Lasso.\n",
    "\n",
    "- Compare and contrast between Lasso-Ridge-Linear models.\n",
    "\n",
    "- Apply Lasso and Ridge with sklearn and understand the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Review \n",
    "## Review Linear Regression once again:\n",
    "![imune](https://media3.giphy.com/media/l4FGBBs6fjtZXzXCE/giphy.gif?cid=ecf05e47dbb74b8bcd3cf3aafbc86e24452503799b466084&rid=giphy.gif)\n",
    "\n",
    "\n",
    "__Linear Model__\n",
    "\n",
    "\n",
    "$$ Y = w_{0} + w_{1}X_1 + w_{2}X_{2} + \\cdots + w_{p}X_{p} + \\varepsilon $$\n",
    "\n",
    " - We train model to understand the paramaters $w_{i}$ \n",
    " \n",
    " - Use linear algebra or gradient descent to find parameters to minimize:\n",
    " \n",
    " Note that the predictions are given by:\n",
    " \n",
    " $$ \\hat{y}_{i} =  w_{0} + w_{1}X_{i1} + w_{2}X_{i2} + \\cdots + w_{p}X_{i_p}$$\n",
    " \n",
    " Therefore individual errors are given by:\n",
    " \n",
    " $$ e_{i} = y_{i} - \\hat{y}_{i} $$\n",
    " \n",
    " As a result, the residual sum of squares can be expressed as:\n",
    " \n",
    " $$ RSS(\\boldsymbol{w}) = \\sum\\limits_{i=0}^{N} e_{i}^{2}$$\n",
    " \n",
    " \n",
    " \n",
    " $$ J(\\boldsymbol{w}) = \\sum\\limits_{i=0}^{N} (y_{i} - w_{0} - w_{1}X_{i1} - w_{2}X_{i2} - \\cdots - w_{p}X_{i_p})^{2} $$\n",
    " \n",
    " And this equation can be written in short hand as:\n",
    " \n",
    " $$ J(\\boldsymbol{w}) = \\rvert \\boldsymbol{y} - X \\boldsymbol{w} \\rvert^{2} $$\n",
    " \n",
    " or with betas as we are used to seeing them:\n",
    " \n",
    " $$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(b_jx_{ij} + b))^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What methods or metrics have you learned to assess model fit?\n",
    "\n",
    "\n",
    "## Another Question: How do those metrics feel about adding more variables to the model?\n",
    "\n",
    "![schitts](https://media1.giphy.com/media/fXtGlVSI2ZB2E1JO0b/giphy.gif?cid=ecf05e47f55093929090866ffd59873c647521e25e164830&rid=giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "<a name=\"review\"></a>\n",
    "\n",
    "\n",
    "\n",
    "[__Overfitting - Underfitting__](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "<img src=\"underfitting_overfitting.png\" alt=\"Bias-Variance\" style=\"width: 500px;\"/>\n",
    "\n",
    "[__Bias - Variance Trade-Off__](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "<img src=\"bias_variance_trade_off.png\" alt=\"Bias-Variance\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Tools:\n",
    "- AIC & BIC to compare models\n",
    "- Regularization to produce a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: AIC & BIC\n",
    "## Calculating AIC and BIC \n",
    "AIC and BIC are information criteria for evaluating model performance. These measures compute the goodness of fit with the estimated parameters, but apply a penalty function on the number of parameters in the model.\n",
    "\n",
    "\n",
    "The BIC is also known as the _Schwarz information criterion (abrv. SIC)_ or the Schwarz-Bayesian information criteria. The AIC is also known as the Akaike information criterion. Both criterion were developed in the 1970s.\n",
    "\n",
    "- AIC is defined as: $2k - 2log(L)$\n",
    "- BIC is defined as: $klog(n) - 2log(L)$  \n",
    "\n",
    "$n$ = sample size <br>\n",
    "$k$ = variables in model <br>\n",
    "$L$ = log of sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def aic(y, y_pred, k):\n",
    "    resid = y - y_pred\n",
    "    sse = (resid**2).sum()\n",
    "    AIC = 2*k - 2*np.log(sse)\n",
    "    \n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(y, y_pred, k, n):\n",
    "    resid = y - y_pred\n",
    "    sse = (resid**2).sum()\n",
    "    BIC = np.log(n) + n*np.log(sse/n)\n",
    "    \n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC & BIC in comparsion\n",
    "\n",
    "Both AIC and BIC are only useful in comparing the performance of two different model specifications and **cannot** be used on their own. \n",
    "\n",
    "_**Lower**_ AIC and BIC are better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularization\n",
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "\n",
    "- Why?\n",
    "\n",
    "    - Reduces complexity\n",
    "    \n",
    "    - Reduce the chance of overfitting.\n",
    "    \n",
    "    - Reduces model's variance at the expense of introducing small bias\n",
    "    \n",
    "    - Increases model's interprettability.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Ridge regularization (L2 Norm)\n",
    "![ridge](https://media2.giphy.com/media/3Aie9MmJ0klyM/giphy.gif?cid=ecf05e472724cf8b955fe4da2a1050dd8865bec22b629736&rid=giphy.gif)\n",
    "\n",
    "Instead of minimizing $J(w)$ (least squares method), we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p} w_{i}^{2} $$\n",
    "\n",
    "Function is in sklearn as [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "The **goal** of ridge regression is to find  the right alpha that best manages multicolinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression applies a penalizing parameter $\\lambda$ *slope* $^2$, such that a small bias will be introduced to the entire model depending on the value of $\\lambda$, which is called a *hyperparameter*. \n",
    "\n",
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(b_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p b_j^2$$\n",
    "\n",
    "The result of applying such a penalizing parameter to the cost function, resulting a different regression model that minimizing the residual sum of square **and** the term $\\lambda \\sum_{j=1}^p b_j^2$. \n",
    "\n",
    "The Ridge regression improves the fit of the original regression line by introducing some bias/changing the slope and intercept of the original line. Recall the way we interpret a regression model Y = mx + b: with every unit increase in x, the outcome y increase by m unit. Therefore, the bigger the coefficient m is, the more the outcome is subjected to changes in predictor x. Ridge regression works by reducing the magnitude of the coefficient m and therefore reducing the effect the predictors have on the outcome. Let's look at a simple example.\n",
    "\n",
    "The ridge regression penalty term contains all of the coefficients squared from the original regression line except for the intercept term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Lasso regularization (L1 Norm)\n",
    "![lasso](https://media3.giphy.com/media/wRKeX8o1eIxxu/giphy.gif?cid=ecf05e47a70490d9dee94f19dd8876390c0ef88243356922&rid=giphy.gif)\n",
    "\n",
    "Instead of minimizing $J(\\boldsymbol{\\omega})$, we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p}| w_{i} | $$\n",
    "\n",
    "Function in skelarn as [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "The **goal** of lasso regression is to obtain the subset of predictors and increase model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is very similar to Ridge regression except for one difference - the penalty term is not squared but the absolute values of the coefficients muliplied by lambda, expressed by:\n",
    "\n",
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(b_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid b_j \\mid$$\n",
    "\n",
    "The biggest difference in Ridge and Lasso is that Lasso simultaneously performs variable selection: some coefficients are shrunk to 0, rendering them nonexistence in the original regression model. Therefore, Lasso regression performs very well when you have higher dimensional dataset where some predictors are useless; whereas Ridge works best when all the predictors are needed. \n",
    "\n",
    "<img src=\"https://media.giphy.com/media/AWeYSE0qgpk76/giphy.gif\" width= \"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression # Ridge and Lasso imported\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "data = sns.load_dataset('mpg')\n",
    "\n",
    "#data = pd.read_csv(\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-2-24-09-ridge-and-lasso-regression/master/auto-mpg.csv\") \n",
    "data = data.sample(50)\n",
    "y = data[[\"mpg\"]]\n",
    "X = data.drop([\"mpg\", \"name\", \"origin\"], axis=1)\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "transformed = scale.fit_transform(X)\n",
    "X = pd.DataFrame(transformed, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>37.0</td>\n",
       "      <td>4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>19.4</td>\n",
       "      <td>81</td>\n",
       "      <td>japan</td>\n",
       "      <td>datsun 210 mpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>30.5</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2190</td>\n",
       "      <td>14.1</td>\n",
       "      <td>77</td>\n",
       "      <td>europe</td>\n",
       "      <td>volkswagen dasher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>23.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>3900</td>\n",
       "      <td>17.4</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>cadillac eldorado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3399</td>\n",
       "      <td>11.0</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge dart custom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>pontiac catalina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2265</td>\n",
       "      <td>15.5</td>\n",
       "      <td>73</td>\n",
       "      <td>europe</td>\n",
       "      <td>fiat 124 sport coupe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2648</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc gremlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>15.5</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4054</td>\n",
       "      <td>14.3</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford country squire (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2158</td>\n",
       "      <td>15.5</td>\n",
       "      <td>73</td>\n",
       "      <td>europe</td>\n",
       "      <td>opel manta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>20.0</td>\n",
       "      <td>6</td>\n",
       "      <td>232.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2914</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc gremlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>12.0</td>\n",
       "      <td>8</td>\n",
       "      <td>383.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>4955</td>\n",
       "      <td>11.5</td>\n",
       "      <td>71</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge monaco (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4382</td>\n",
       "      <td>13.5</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge d200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>26.5</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2565</td>\n",
       "      <td>13.6</td>\n",
       "      <td>76</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford pinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>30.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2051</td>\n",
       "      <td>17.0</td>\n",
       "      <td>77</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet chevette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6</td>\n",
       "      <td>156.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2930</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "      <td>japan</td>\n",
       "      <td>toyota mark ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>41.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2144</td>\n",
       "      <td>14.7</td>\n",
       "      <td>80</td>\n",
       "      <td>europe</td>\n",
       "      <td>vw rabbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>34.1</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1985</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81</td>\n",
       "      <td>japan</td>\n",
       "      <td>mazda glc 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>20.0</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3651</td>\n",
       "      <td>17.7</td>\n",
       "      <td>76</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge aspen se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2074</td>\n",
       "      <td>19.5</td>\n",
       "      <td>71</td>\n",
       "      <td>europe</td>\n",
       "      <td>peugeot 304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>18.5</td>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3645</td>\n",
       "      <td>16.2</td>\n",
       "      <td>76</td>\n",
       "      <td>usa</td>\n",
       "      <td>pontiac ventura sj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3158</td>\n",
       "      <td>19.5</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>35.7</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1915</td>\n",
       "      <td>14.4</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge colt hatchback custom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>35.0</td>\n",
       "      <td>4</td>\n",
       "      <td>122.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>15.1</td>\n",
       "      <td>80</td>\n",
       "      <td>europe</td>\n",
       "      <td>triumph tr7 coupe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>18.0</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2124</td>\n",
       "      <td>13.5</td>\n",
       "      <td>73</td>\n",
       "      <td>japan</td>\n",
       "      <td>maxda rx3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>20.5</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3430</td>\n",
       "      <td>17.2</td>\n",
       "      <td>78</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth volare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>3264</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth valiant custom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>88.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2065</td>\n",
       "      <td>14.5</td>\n",
       "      <td>71</td>\n",
       "      <td>europe</td>\n",
       "      <td>fiat 124b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1985</td>\n",
       "      <td>16.4</td>\n",
       "      <td>77</td>\n",
       "      <td>japan</td>\n",
       "      <td>subaru dl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2228</td>\n",
       "      <td>14.0</td>\n",
       "      <td>71</td>\n",
       "      <td>japan</td>\n",
       "      <td>toyota corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1834</td>\n",
       "      <td>19.0</td>\n",
       "      <td>71</td>\n",
       "      <td>europe</td>\n",
       "      <td>volkswagen model 111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>18.5</td>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>77</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford granada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4735</td>\n",
       "      <td>11.0</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>chrysler new yorker brougham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>3459</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet nova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>14.5</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>4215</td>\n",
       "      <td>12.8</td>\n",
       "      <td>76</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford gran torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>21.0</td>\n",
       "      <td>4</td>\n",
       "      <td>122.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2226</td>\n",
       "      <td>16.5</td>\n",
       "      <td>72</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford pinto runabout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>16.5</td>\n",
       "      <td>6</td>\n",
       "      <td>168.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3820</td>\n",
       "      <td>16.7</td>\n",
       "      <td>76</td>\n",
       "      <td>europe</td>\n",
       "      <td>mercedes-benz 280s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2542</td>\n",
       "      <td>17.0</td>\n",
       "      <td>74</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet vega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2255</td>\n",
       "      <td>17.7</td>\n",
       "      <td>76</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge colt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>111.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2155</td>\n",
       "      <td>14.8</td>\n",
       "      <td>77</td>\n",
       "      <td>usa</td>\n",
       "      <td>buick opel isuzu deluxe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2220</td>\n",
       "      <td>16.9</td>\n",
       "      <td>76</td>\n",
       "      <td>europe</td>\n",
       "      <td>opel 1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>15.5</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3962</td>\n",
       "      <td>13.9</td>\n",
       "      <td>76</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc matador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>30.7</td>\n",
       "      <td>6</td>\n",
       "      <td>145.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3160</td>\n",
       "      <td>19.6</td>\n",
       "      <td>81</td>\n",
       "      <td>europe</td>\n",
       "      <td>volvo diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>16.5</td>\n",
       "      <td>73</td>\n",
       "      <td>japan</td>\n",
       "      <td>datsun 610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>38.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>16.2</td>\n",
       "      <td>82</td>\n",
       "      <td>japan</td>\n",
       "      <td>datsun 310 gx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>22.0</td>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2511</td>\n",
       "      <td>18.0</td>\n",
       "      <td>72</td>\n",
       "      <td>europe</td>\n",
       "      <td>volkswagen 411 (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>4129</td>\n",
       "      <td>13.0</td>\n",
       "      <td>72</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford galaxie 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>20.2</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3060</td>\n",
       "      <td>17.1</td>\n",
       "      <td>81</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford granada gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>20.2</td>\n",
       "      <td>6</td>\n",
       "      <td>232.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3265</td>\n",
       "      <td>18.2</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc concord dl 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>33.8</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2145</td>\n",
       "      <td>18.0</td>\n",
       "      <td>80</td>\n",
       "      <td>japan</td>\n",
       "      <td>subaru dl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "347  37.0          4          85.0        65.0    1975          19.4   \n",
       "240  30.5          4          97.0        78.0    2190          14.1   \n",
       "298  23.0          8         350.0       125.0    3900          17.4   \n",
       "395  32.0          4         135.0        84.0    2295          11.6   \n",
       "121  15.0          8         318.0       150.0    3399          11.0   \n",
       "8    14.0          8         455.0       225.0    4425          10.0   \n",
       "114  26.0          4          98.0        90.0    2265          15.5   \n",
       "24   21.0          6         199.0        90.0    2648          15.0   \n",
       "290  15.5          8         351.0       142.0    4054          14.3   \n",
       "118  24.0          4         116.0        75.0    2158          15.5   \n",
       "169  20.0          6         232.0       100.0    2914          16.0   \n",
       "42   12.0          8         383.0       180.0    4955          11.5   \n",
       "27   11.0          8         318.0       210.0    4382          13.5   \n",
       "206  26.5          4         140.0        72.0    2565          13.6   \n",
       "237  30.5          4          98.0        63.0    2051          17.0   \n",
       "210  19.0          6         156.0       108.0    2930          15.5   \n",
       "309  41.5          4          98.0        76.0    2144          14.7   \n",
       "349  34.1          4          91.0        68.0    1985          16.0   \n",
       "199  20.0          6         225.0       100.0    3651          17.7   \n",
       "51   30.0          4          79.0        70.0    2074          19.5   \n",
       "201  18.5          6         250.0       110.0    3645          16.2   \n",
       "155  15.0          6         250.0        72.0    3158          19.5   \n",
       "295  35.7          4          98.0        80.0    1915          14.4   \n",
       "335  35.0          4         122.0        88.0    2500          15.1   \n",
       "111  18.0          3          70.0        90.0    2124          13.5   \n",
       "256  20.5          6         225.0       100.0    3430          17.2   \n",
       "152  19.0          6         225.0        95.0    3264          16.0   \n",
       "52   30.0          4          88.0        76.0    2065          14.5   \n",
       "239  30.0          4          97.0        67.0    1985          16.4   \n",
       "31   25.0          4         113.0        95.0    2228          14.0   \n",
       "55   27.0          4          97.0        60.0    1834          19.0   \n",
       "228  18.5          6         250.0        98.0    3525          19.0   \n",
       "94   13.0          8         440.0       215.0    4735          11.0   \n",
       "153  18.0          6         250.0       105.0    3459          16.0   \n",
       "190  14.5          8         351.0       152.0    4215          12.8   \n",
       "61   21.0          4         122.0        86.0    2226          16.5   \n",
       "211  16.5          6         168.0       120.0    3820          16.7   \n",
       "132  25.0          4         140.0        75.0    2542          17.0   \n",
       "185  26.0          4          98.0        79.0    2255          17.7   \n",
       "217  30.0          4         111.0        80.0    2155          14.8   \n",
       "183  25.0          4         116.0        81.0    2220          16.9   \n",
       "189  15.5          8         304.0       120.0    3962          13.9   \n",
       "360  30.7          6         145.0        76.0    3160          19.6   \n",
       "110  22.0          4         108.0        94.0    2379          16.5   \n",
       "385  38.0          4          91.0        67.0    1995          16.2   \n",
       "77   22.0          4         121.0        76.0    2511          18.0   \n",
       "65   14.0          8         351.0       153.0    4129          13.0   \n",
       "365  20.2          6         200.0        88.0    3060          17.1   \n",
       "283  20.2          6         232.0        90.0    3265          18.2   \n",
       "331  33.8          4          97.0        67.0    2145          18.0   \n",
       "\n",
       "     model_year  origin                          name  \n",
       "347          81   japan                datsun 210 mpg  \n",
       "240          77  europe             volkswagen dasher  \n",
       "298          79     usa             cadillac eldorado  \n",
       "395          82     usa                 dodge rampage  \n",
       "121          73     usa             dodge dart custom  \n",
       "8            70     usa              pontiac catalina  \n",
       "114          73  europe          fiat 124 sport coupe  \n",
       "24           70     usa                   amc gremlin  \n",
       "290          79     usa      ford country squire (sw)  \n",
       "118          73  europe                    opel manta  \n",
       "169          75     usa                   amc gremlin  \n",
       "42           71     usa             dodge monaco (sw)  \n",
       "27           70     usa                    dodge d200  \n",
       "206          76     usa                    ford pinto  \n",
       "237          77     usa            chevrolet chevette  \n",
       "210          76   japan                toyota mark ii  \n",
       "309          80  europe                     vw rabbit  \n",
       "349          81   japan                   mazda glc 4  \n",
       "199          76     usa                dodge aspen se  \n",
       "51           71  europe                   peugeot 304  \n",
       "201          76     usa            pontiac ventura sj  \n",
       "155          75     usa                 ford maverick  \n",
       "295          79     usa   dodge colt hatchback custom  \n",
       "335          80  europe             triumph tr7 coupe  \n",
       "111          73   japan                     maxda rx3  \n",
       "256          78     usa               plymouth volare  \n",
       "152          75     usa       plymouth valiant custom  \n",
       "52           71  europe                     fiat 124b  \n",
       "239          77   japan                     subaru dl  \n",
       "31           71   japan                 toyota corona  \n",
       "55           71  europe          volkswagen model 111  \n",
       "228          77     usa                  ford granada  \n",
       "94           73     usa  chrysler new yorker brougham  \n",
       "153          75     usa                chevrolet nova  \n",
       "190          76     usa              ford gran torino  \n",
       "61           72     usa           ford pinto runabout  \n",
       "211          76  europe            mercedes-benz 280s  \n",
       "132          74     usa                chevrolet vega  \n",
       "185          76     usa                    dodge colt  \n",
       "217          77     usa       buick opel isuzu deluxe  \n",
       "183          76  europe                     opel 1900  \n",
       "189          76     usa                   amc matador  \n",
       "360          81  europe                  volvo diesel  \n",
       "110          73   japan                    datsun 610  \n",
       "385          82   japan                 datsun 310 gx  \n",
       "77           72  europe           volkswagen 411 (sw)  \n",
       "65           72     usa              ford galaxie 500  \n",
       "365          81     usa               ford granada gl  \n",
       "283          79     usa              amc concord dl 6  \n",
       "331          80   japan                     subaru dl  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform test-train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpenalized Linear Regression Coefficients are:[[ -4.94696165   1.1290167    8.27870166 -19.25020734   0.36206206\n",
      "   14.28708231]]\n",
      "Unpenalized Linear Regression Intercept:[23.15228868]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lin.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lin.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Coefficients are:[-3.53483959 -0.         -0.         -8.71029094  0.          8.23022506]\n",
      "Lasso Linear Regression Intercept:[24.69796891]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Coefficients are:[[-4.71261657 -3.72183827 -0.69059297 -8.2006064  -1.73264034 11.33322564]]\n",
      "Ridge Linear Regression Intercept:[25.7478184]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_h_ridge_train.shape)\n",
    "print(y_h_ridge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_h_lasso_train))\n",
    "print(type(y_h_ridge_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Residual for Ridge, Lasso, and Unpenalized Regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Ridge Model mpg    477.427428\n",
      "dtype: float64\n",
      "Test Error Ridge Model mpg    149.749226\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Lasso Model mpg    648.676787\n",
      "dtype: float64\n",
      "Test Error Lasso Model mpg    187.993173\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Unpenalized Linear Model mpg    428.76896\n",
      "dtype: float64\n",
      "Test Error Unpenalized Linear Model mpg    140.065272\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2c: Crossvalidation to Optimize the Regularization Hyperparameter\n",
    "\n",
    "The regularization strength could sensibly be any nonnegative number, so there's no way to check \"all possible\" values. It's often useful to try several values that are different orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4fb53ea5434b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# upate the names of your datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_processed' is not defined"
     ]
    }
   ],
   "source": [
    "alphas = [1, 10, 100, 1000, 10000]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha, random_state=42)\n",
    "    # upate the names of your datasets\n",
    "    rr.fit(X_train_processed, y_train)\n",
    "    train_score = rr.score(X_train_processed, y_train)\n",
    "    test_score = cross_val_score(rr, X_test_processed, y_test).mean()\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots()\n",
    "plt.xscale('log')\n",
    "plt.title('Ridge $R^2$ as a function of regularization strength')\n",
    "ax.set_xlabel('Regularization strength $\\lambda$')\n",
    "ax.set_ylabel('$R^2$')\n",
    "ax.plot(alphas, train_scores, label='train')\n",
    "ax.plot(alphas, test_scores, label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Practice\n",
    "\n",
    "__Your Turn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dc3d3ca643d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mdummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'species'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'island'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(),\n\u001b[0;32m     24\u001b[0m                          index=X_train.index)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, handle_unknown)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mX_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mXi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             Xi = check_array(Xi, ensure_2d=False, dtype=None,\n\u001b[1;32m---> 61\u001b[1;33m                              force_all_finite=needs_validation)\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mX_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 578\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input contains NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "birds = sns.load_dataset('penguins')\n",
    "\n",
    "# For simplicity's sake we'll limit our analysis to the numeric columns.\n",
    "\n",
    "numeric = birds[['culmen_length_mm', 'culmen_depth_mm',\n",
    "                 'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "\n",
    "# We'll drop the rows with null values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(birds.drop('body_mass_g',\n",
    "                                                              axis=1),\n",
    "                                                   birds['body_mass_g'],\n",
    "                                                   random_state=42)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "dummies = ohe.fit_transform(X_train[['species', 'island', 'sex']])\n",
    "dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                         index=X_train.index)\n",
    "X_train_df = pd.concat([X_train[['culmen_length_mm', 'culmen_depth_mm',\n",
    "                                'flipper_length_mm']], dummies_df], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try at least 3 different model specifications, using polynomials, interaction terms, etc\n",
    "- Group 1: Use AIC and BIC to pick your \"best\" model specification using Linear Regression\n",
    "- Group 2: Use Ridge AND experiement with different values of alpha to find the \"best\" model. Compare MSE in train and test for each model specification.\n",
    "- Group 3: Use Lasso AND experiment with different values of alpha to find the \"best\" model. Compare MSE in train and test for each model specification.\n",
    "- Report your observations.\n",
    "- A graph comparing your findings and suggesting a model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Summary\n",
    "\n",
    "#### Effect of $\\alpha$ in Lasso and Ridge\n",
    "\n",
    "<img src=\"lasso_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"ridge_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<a name='questions'></a>\n",
    "### Questions\n",
    "\n",
    "\n",
    "\n",
    "Q. Should I do normalization for Lasso or Ridge?\n",
    "\n",
    "A. Yes? Why?\n",
    "\n",
    "Q. When we know that Ridge and Lasso is better than vanilla linear regression?\n",
    "\n",
    "A. High variation in your model --> Colinearity and too many variables.\n",
    "\n",
    "Q. How do we know whether we should choose Lasso or Ridge?\n",
    "\n",
    "A. Most of the time they perform very similar but Lasso has the feature selection property, ridge doesn't have this.\n",
    "\n",
    "Q: How do we choose $\\lambda$?\n",
    "\n",
    "A. [sklearn gridsearch](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) for small models or random grid search for bigger models.\n",
    "\n",
    "#### Appendix\n",
    "<a name='appendix'></a>\n",
    "\n",
    "Here I would like to add some reading material that I found useful while working with the code.\n",
    "\n",
    "\n",
    "-  [On ridge and lasso](https://bradleyboehmke.github.io/HOML/regularized-regression.html)\n",
    "\n",
    "- [pd.get_dummies or OneHotEncoder? - Read second answer](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons)\n",
    "\n",
    "- [On dummy variable trap](https://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
    "\n",
    "- [sklearn.preprocessing.PolynomialFeatures documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "- [A great notebook on Lasso and Ridge](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "- [Another good blog post on Lasso and Ridge](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)\n",
    "\n",
    "- Learn.co -- Section-28 Lasso-Ridge\n",
    "\n",
    "- [Toward Datascience Article](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "\n",
    "- [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) 2.2.2 The Bias-Variance Trade-off and 6.2 Shrinkage Methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Image Sources in order of appearance: \n",
    "- https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\n",
    "\n",
    "- https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
